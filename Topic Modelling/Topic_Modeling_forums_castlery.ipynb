{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will need the stopwords from NLTK and spacy’s en model for text pre-processing.\n",
    "Later, we will be using the spacy model for lemmatization.\n",
    "\n",
    "Lemmatization is nothing but converting a word to its root word.\n",
    "For example: the lemma of the word ‘machines’ is ‘machine’.\n",
    "\n",
    "Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on.\n",
    "\n",
    "'''\n",
    "\n",
    "# Run in python console\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# nltk.download('stopwords') # You already did this in a previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim (Topic Modeling Pacakge)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.gensim_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bf0684677164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plotting tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim_models\u001b[0m  \u001b[1;31m# don't skip this\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.gensim_models'"
     ]
    }
   ],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['scanteaksg','scanteak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Body</th>\n",
       "      <th>Reply to</th>\n",
       "      <th>Message Replying to</th>\n",
       "      <th>Permalink</th>\n",
       "      <th>Score</th>\n",
       "      <th>Subpage/Subreddit</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lord___Shaxx</td>\n",
       "      <td>We got a nice dining table from Castlery. A bi...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>/r/singapore/comments/l2y5er/rsingapore_random...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>singapore</td>\n",
       "      <td>23/1/2021 02:35</td>\n",
       "      <td>2021</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>WittyKap0</td>\n",
       "      <td>Depends on what you consider decent\\r\\n \\r\\n \\...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>/r/singapore/comments/k5jlln/rsingapore_random...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>singapore</td>\n",
       "      <td>3/12/2020 02:07</td>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>coff33mug</td>\n",
       "      <td>castlery, crate&amp;amp;barrels</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>/r/singapore/comments/jpzdte/rsingapore_random...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>singapore</td>\n",
       "      <td>8/11/2020 13:03</td>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gcross2110</td>\n",
       "      <td>I use a kitchen table purchased from castlery....</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>/r/singapore/comments/hg8nn0/recommendations_f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>singapore</td>\n",
       "      <td>27/6/2020 04:21</td>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ryecotta</td>\n",
       "      <td>You could try:\\r\\n \\r\\n \\r\\n \\r\\n 1. Carousell...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>/r/singapore/comments/eyruuf/best_websites_for...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>singapore</td>\n",
       "      <td>4/2/2020 16:27</td>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Author  \\\n",
       "0           0  Lord___Shaxx   \n",
       "1           1     WittyKap0   \n",
       "2           2     coff33mug   \n",
       "3           3    gcross2110   \n",
       "4           4      ryecotta   \n",
       "\n",
       "                                                Body Reply to  \\\n",
       "0  We got a nice dining table from Castlery. A bi...        -   \n",
       "1  Depends on what you consider decent\\r\\n \\r\\n \\...        -   \n",
       "2                        castlery, crate&amp;barrels        -   \n",
       "3  I use a kitchen table purchased from castlery....        -   \n",
       "4  You could try:\\r\\n \\r\\n \\r\\n \\r\\n 1. Carousell...        -   \n",
       "\n",
       "  Message Replying to                                          Permalink  \\\n",
       "0                   -  /r/singapore/comments/l2y5er/rsingapore_random...   \n",
       "1                   -  /r/singapore/comments/k5jlln/rsingapore_random...   \n",
       "2                   -  /r/singapore/comments/jpzdte/rsingapore_random...   \n",
       "3                   -  /r/singapore/comments/hg8nn0/recommendations_f...   \n",
       "4                   -  /r/singapore/comments/eyruuf/best_websites_for...   \n",
       "\n",
       "   Score Subpage/Subreddit        TimeStamp  Year  Source  \n",
       "0    1.0         singapore  23/1/2021 02:35  2021  Reddit  \n",
       "1    1.0         singapore  3/12/2020 02:07  2020  Reddit  \n",
       "2    1.0         singapore  8/11/2020 13:03  2020  Reddit  \n",
       "3    1.0         singapore  27/6/2020 04:21  2020  Reddit  \n",
       "4    1.0         singapore   4/2/2020 16:27  2020  Reddit  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will be using the 20-Newsgroups dataset for this exercise.\n",
    "This version of the dataset contains about 11k newsgroups posts from \n",
    "20 different topics. This is available as newsgroups.json.\n",
    "'''\n",
    "# Import Dataset\n",
    "df = pd.read_csv('../Clean Data/clean_consolidated_castlery (forum).csv', encoding='utf-8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tanjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We got a nice dining table from Castlery. A bit pricy, but quality materials. I know they have other furniture like sofas too.', \"Depends on what you consider decent\\r\\n \\r\\n \\r\\n \\r\\n If you want solid wood base probably need 3-4k and up. \\r\\n \\r\\n \\r\\n \\r\\n Can look at King living for something cheaper in the 1k+ range\\r\\n \\r\\n \\r\\n \\r\\n Those cheap taobao couch at places like castlery can cost 500-1k but durability and comfort is super low. \\r\\n \\r\\n \\r\\n \\r\\n You can probably get some cheaper sofas from the big furniture stores but personally they look ugly and old fashioned, comfort isn't great either. But really depends on personal taste\", 'castlery, crate&amp;barrels', 'I use a kitchen table purchased from castlery. Got it on sale. Its around 160cm Long so perfect for a large desk and very sturdy.', 'You could try:\\r\\n \\r\\n \\r\\n \\r\\n 1. Carousell - sort of like Ebay. Lotsa second hand IKEA or even vintage items, lightly used and very cheaply. I even got my dining set free from another Singaporean who was migrating. \\r\\n \\r\\n \\r\\n \\r\\n 2. Hock Siong - physical shop with limited one-off pieces. Check out their IG here: https://instagram.com/hocksiong\\r\\n \\r\\n \\r\\n \\r\\n 3. Second Charm - also a physical shop with one-off second hand vintage pieces: https://instagram.com/second.charm\\r\\n \\r\\n \\r\\n \\r\\n 4. Castlery and Hipvan - Lotsa Singaporeans get their furniture there. They are fairly well designed and affordable. Sorta like IKEA. \\r\\n \\r\\n \\r\\n \\r\\n 5. Noden - second hand well curated Scandinavian furniture and sideboards. However, they are not cheap save for one or two smaller items. Got a set of nesting tables from them years ago and they are still going strong, beautifully.', 'Castlery, ikea?\\r\\n \\r\\n \\r\\n \\r\\n Can try one of these too, and get a frame elsewhere https://blog.moneysmart.sg/shopping/cheap-mattress-online/', \"Ethnicraft bed frame is pretty good, Crate and Barrel is quite good value relative to what's available, especially on discount. Kuhl Home/Conde House is also nice but expensive. \\r\\n \\r\\n \\r\\n \\r\\n Technically everything is imported, we don't have much woodworking industry here but I do understand what you mean. Pretty much all the usual furniture places like castlery, star furniture, etc are cheap and crappy\", \"Crate and barrel are well priced locally compared to most if not all the recommended local hipster shops like castlery and it's ilk basically reselling cheap crap. \\r\\n \\r\\n \\r\\n \\r\\n I scoured 10-20 shops for a decent couch before finally getting one from C&amp;B. The other options I considered were Conde house and Juul home which were like 2-4x the price.\", \"For couch, look at the Kuhl Home Juul range. Most comfy couch I tried but it would have been about 8-9k with the options I wanted. Crate and barrel can be comfy too if you get the right models. I also liked the full grain leather Ikea couch which was 4-5k but the construction was really shitty, what a waste. \\r\\n \\r\\n \\r\\n \\r\\n Of course I assume you care about aesthetics too... Otherwise you could probably find a comfy and ugly couch from somewhere like idk Harvey Norman or king living \\r\\n \\r\\n \\r\\n \\r\\n Just don't bother with the hipster places like castlery. You can't have it look nice and be comfortable for like $1000\", 'Might want to check out the bags from Castlery. Bought one and loved it!', 'Recently moved here! Whats a good place to get a bed/mattress and sofas? Already checked our Ikea, furniture mall (too expensive), castlery (liked stuff there), tan boon liat (again too expensive). What else did I miss?', 'Got most of my stuff from Commune, Castlery, Hipvan, Etch and Bolts, Born in Colour and Scanteak! If you like marble, can consider Martlewood amd Greyhammer too. The Tan Boon Liat building also has many furniture stores', 'Everyone in this thread has been awesome but his is outstanding. I have discovered the property guru and some of the furniture places. I love second hand stuff so I am excited about checking out castlery. I am moving for work and itd be a three year assignment. My ideal situation is moving everything when I fly out there so I will definitely have to talk to the airline. My work is covering moving costs, so that is nice. Its really daunting to think about buying all new stuff and I dont really want to bring anything besides clothes and a few other items. I think renting a filly furnished apt is the way to go. \\r\\n \\r\\n \\r\\n \\r\\n Work will be around raffles or 10ish miles to the east. Im looking into condos in that area as well. Very exciting to see whats out there but Ive heard I really shouldnt look until the month before I move since thats how rentals work. \\r\\n \\r\\n \\r\\n \\r\\n Very excited to move though!', 'Try Etch and Bolts, Crate and Barrel, Hipvan and Castlery?', 'castlery?', 'Hipvan, castlery, comfort furniture', 'Depends on what is your budget or what you mean by recommended. Places like Lush, Comfort Furniture, Hip Van they get their stuff mostly from Malaysian factories. Pull aside the product branding (e.g Nesthouz) and you get....Hin Lim Sdn Bhd (http://www.hinlim.com/). **In other words, overpriced Malaysian furniture being sold to Singaporeans under fancy brands. Do yourself a favor and avoid them.** \\r\\n \\r\\n  \\r\\n \\r\\n \\r\\n \\r\\n Teak furniture is also similar as teak has many grades. Scanteak is reasonably price for their stuff and I got no complaints so far. There is also Mountain Teak, Originals and Teak Avenue but those are expensive shit. I am using a bedframe from Widhardja. The quality is rough, but the wood is ok so far. Have another bedframe from Castlery and it was ok.\\r\\n \\r\\n \\r\\n \\r\\n Ikea, I have no experience with their big items (beds, sofas or kitchen) but simple small stuff was ok. Feedback on those big items tend to be mixed.', 'I had a bad experience with them. Would suggest hipvan or castlery instead, these 2 have been reliable for me so far.', \"If you are into the Scandinavian look, try Noden Collective (nodenhome.com) or Mobler Singapore (moblersingapore.com). MUJI too, if you like the minimalist Japanese look and they do have quite frequent sales these days. Secondcharm if you're a fan of vintage or refurbished items!\\r\\n \\r\\n \\r\\n \\r\\n Harvey Norman at Millenia Walk is worth a look too as they carry some special brands there not at their other stores.\\r\\n \\r\\n \\r\\n \\r\\n Tbqh, my husband is a furniture maker but we still did our rounds at the usual suspects (IKEA, castlery etc) when we got our place a couple of months ago. But it was still a hard to find the right quality items!\", \"There are a ton of shops in the Tan Boon Liat building in Tiong Bahru for varying budgets on the mid to high end. Some good quality and choices - if nothing else will give you some ideas. \\r\\n \\r\\n \\r\\n \\r\\n The Furniture Mall is pretty good too, but quality can be iffy, so just make sure you know what you are buying. \\r\\n \\r\\n \\r\\n \\r\\n I think Castlery is good for tables and chairs, but not so much for sofas. \\r\\n \\r\\n \\r\\n \\r\\n Check out Commune too. They have a big shop in Melina Walk (across from Harvey Norman, which has lots of choices too, but I don't personal love their look). Commune is a local company with some nice stuff.\"]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As you can see there are many emails, newline and extra spaces that is quite distracting.\n",
    "Let’s get rid of them using regular expressions.\n",
    "'''\n",
    "# Convert to list\n",
    "data = df.Body.values.tolist()\n",
    "new_data = []\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub(r'[^\\x00-\\x7F]+', '', sent) for sent in data]\n",
    "\n",
    "# # Remove new line characters\n",
    "# data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# # Remove distracting single quotes\n",
    "# data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "print(data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis.gensim' has no attribute 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-384bd64afa9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32myield\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# deacc=True removes punctuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdata_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlist_of_words\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-384bd64afa9c>\u001b[0m in \u001b[0;36msent_to_words\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msent_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32myield\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# deacc=True removes punctuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdata_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis.gensim' has no attribute 'utils'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After removing the emails and extra spaces, the text still looks messy.\n",
    "It is not ready for the LDA to consume. You need to break down each sentence\n",
    "into a list of words through tokenization, while clearing up all the messy \n",
    "text in the process.\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and \n",
    "unnecessary characters altogether.\n",
    "\n",
    "Gensim’s simple_preprocess() is great for this.\n",
    "Additionally I have set deacc=True to remove the punctuations.\n",
    "'''\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[0])\n",
    "for list_of_words in data_words:\n",
    "    for word in list_of_words:\n",
    "        if word.lower() in words or not word.isalpha():\n",
    "            list_of_words.remove(word)\n",
    "    \n",
    "print(data_words[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['heart_suit'], ['pleasure', 'want', 'go', 'it', 'so', 'bz', 'this'], ['giftsnsuchsg', 'you', 'much', 'your', 'wishes', 'mrs', 'chan'], ['the'], ['christmas', 'handmadecard', 'scanteaksg', 'papercrafts', 'crafts', 'uniquegrabs'], ['hermeswechat', 'aifeihermes'], ['trickeye', 'chistmas', 'snowglobe'], ['scanteaksmile'], ['looks', 'we', 'be', 'business', 'future', 'haha'], ['my', 'thoughts', 'polka', 'chair', 'be', 'ideal', 'haha'], ['cherrychoo', 'help', 'of', 'and', 'dots', 'you', 'have', 'pink', 'think'], ['polka', 'dots', 'eyes'], ['do', 'think', 'this', 'stephlimmss'], ['eternalgluck', 'zhong', 'bu', 'zhong', 'yong'], ['hellonoraa', 'coffee', 'though'], ['it'], ['hpu', 'theminiloft'], ['love'], ['sallytumultuous', 'pleasure', 'ours', 're', 'you', 'happy', 'andy', 'and', 'definitely', 'on', 'compliments', 'him', 'great', 'hugging_face'], ['you', 'the', 'scanteaksg', 'thanks', 'the', 'staff', 'park', 'forgive', 'if', 'got', 'name', 'up', 'believe', 'name', 'andy', 'him', 'say']]\n"
     ]
    }
   ],
   "source": [
    "data_words = [x for x in data_words if x]\n",
    "\n",
    "print(data_words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heart_suit']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Bigrams are two words frequently occurring together in the document.\n",
    "Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, \n",
    "quadgrams and more.\n",
    "\n",
    "The two important arguments to Phrases are min_count and threshold.\n",
    "The higher the values of these param, the harder it is for words to be combined to bigrams.\n",
    "'''\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, \n",
    "make bigrams and lemmatization and call them sequentially.\n",
    "'''\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['heart_suit']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let’s call the functions in order.\n",
    "'''\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus.\n",
    "Let’s create them.\n",
    "'''\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGensim creates a unique id for each word in the document.\\nThe produced corpus shown above is a mapping of (word_id, word_frequency).\\n\\nFor example, (0, 1) above implies, word id 0 occurs once in the first document.\\n\\nLikewise, word id 1 occurs twice and so on.\\n\\nThis is used as the input by the LDA model.\\n\\nIf you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\\n\\nid2word[0]\\n'addition'\\n\\nOr, you can see a human-readable form of the corpus itself.\\n\\n# Human readable format of corpus (term-frequency)\\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\\n\\n[[('addition', 1),\\n  ('anyone', 2),\\n  ('body', 1),\\n  ('bricklin', 1),\\n  ('bring', 1),\\n  ('call', 1),\\n  ('car', 5),\\n  ('could', 1),\\n  ('day', 1),\\n  ('door', 2),\\n  ('early', 1),\\n  ('engine', 1),\\n  ('enlighten', 1),\\n  ('front_bumper', 1),\\n  ('maryland_college', 1),\\n  (..truncated..)]]\\n\\n\\nAlright, without digressing further let’s jump back on track with the next step:\\n\\nBuilding the topic model.\\n\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Gensim creates a unique id for each word in the document.\n",
    "The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document.\n",
    "\n",
    "Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\n",
    "\n",
    "id2word[0]\n",
    "'addition'\n",
    "\n",
    "Or, you can see a human-readable form of the corpus itself.\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "[[('addition', 1),\n",
    "  ('anyone', 2),\n",
    "  ('body', 1),\n",
    "  ('bricklin', 1),\n",
    "  ('bring', 1),\n",
    "  ('call', 1),\n",
    "  ('car', 5),\n",
    "  ('could', 1),\n",
    "  ('day', 1),\n",
    "  ('door', 2),\n",
    "  ('early', 1),\n",
    "  ('engine', 1),\n",
    "  ('enlighten', 1),\n",
    "  ('front_bumper', 1),\n",
    "  ('maryland_college', 1),\n",
    "  (..truncated..)]]\n",
    "\n",
    "\n",
    "Alright, without digressing further let’s jump back on track with the next step:\n",
    "\n",
    "Building the topic model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We have everything required to train the LDA model.\n",
    "In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics.\n",
    "According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk.\n",
    "\n",
    "update_every determines how often the model parameters should be updated and \n",
    "passes is the total number of training passes.\n",
    "'''\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.142*\"eye\" + 0.022*\"cantik\" + 0.020*\"sparkle\" + 0.012*\"red_heart\" + '\n",
      "  '0.012*\"interior\" + 0.010*\"hugging_face\" + 0.010*\"much\" + 0.009*\"pm\" + '\n",
      "  '0.008*\"furniture\" + 0.008*\"want\"'),\n",
      " (1,\n",
      "  '0.041*\"sghome\" + 0.041*\"feelathome\" + 0.040*\"furniturestyle\" + '\n",
      "  '0.040*\"sginterior\" + 0.040*\"homeinspiration\" + 0.040*\"homeinspo\" + '\n",
      "  '0.040*\"homestyle\" + 0.040*\"homestyling\" + 0.040*\"interiorsg\" + '\n",
      "  '0.040*\"sghomedecor\"'),\n",
      " (2,\n",
      "  '0.031*\"love\" + 0.023*\"look\" + 0.019*\"year\" + 0.015*\"think\" + 0.013*\"sofa\" + '\n",
      "  '0.012*\"heart_suit\" + 0.012*\"cover\" + 0.011*\"come\" + 0.009*\"thank\" + '\n",
      "  '0.009*\"machine\"')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe output will look like this...\\n\\n\\n[(0,\\n  \\'0.016*\"car\" + 0.014*\"power\" + 0.010*\"light\" + 0.009*\"drive\" + 0.007*\"mount\" \\'\\n  \\'+ 0.007*\"controller\" + 0.007*\"cool\" + 0.007*\"engine\" + 0.007*\"back\" + \\'\\n  \\'0.006*\"turn\"\\'),\\n (1,\\n  \\'0.072*\"line\" + 0.066*\"organization\" + 0.037*\"write\" + 0.032*\"article\" + \\'\\n  \\'0.028*\"university\" + 0.027*\"nntp_post\" + 0.026*\"host\" + 0.016*\"reply\" + \\'\\n  \\'0.014*\"get\" + 0.013*\"thank\"\\'),\\n (2,\\n  \\'0.017*\"patient\" + 0.011*\"study\" + 0.010*\"slave\" + 0.009*\"wing\" + \\'\\n  \\'0.009*\"disease\" + 0.008*\"food\" + 0.008*\"eat\" + 0.008*\"pain\" + \\'\\n  \\'0.007*\"treatment\" + 0.007*\"syndrome\"\\'),\\n (3,\\n  \\'0.013*\"key\" + 0.009*\"use\" + 0.009*\"may\" + 0.007*\"public\" + 0.007*\"system\" + \\'\\n  \\'0.007*\"order\" + 0.007*\"government\" + 0.006*\"state\" + 0.006*\"provide\" + \\'\\n  \\'0.006*\"law\"\\'),\\n  \\n  so on...\\n\\n\\n\\nHow to interpret this?\\n\\nTopic 0 is a represented as _\\n\\n0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” \\n+ 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” \\n+ 0.007“back” + ‘0.006“turn”.\\n\\nIt means the top 10 keywords that contribute to this topic are:\\n‘car’, ‘power’, ‘light’.. and so on\\nand the weight of ‘car’ on topic 0 is 0.016.\\n\\n--> The weights reflect how important a keyword is to that topic.\\n\\nLooking at these keywords, can you guess what this topic could be?\\nYou may summarise it either are ‘cars’ or ‘automobiles’.\\n\\nLikewise, can you go through the remaining topic keywords and judge what the topic is?\\nhttps://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords.png\\n'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The above LDA model is built with 20 different topics where each topic is a \n",
    "combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of \n",
    "each keyword using lda_model.print_topics() as shown next.\n",
    "'''\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "'''\n",
    "The output will look like this...\n",
    "\n",
    "\n",
    "[(0,\n",
    "  '0.016*\"car\" + 0.014*\"power\" + 0.010*\"light\" + 0.009*\"drive\" + 0.007*\"mount\" '\n",
    "  '+ 0.007*\"controller\" + 0.007*\"cool\" + 0.007*\"engine\" + 0.007*\"back\" + '\n",
    "  '0.006*\"turn\"'),\n",
    " (1,\n",
    "  '0.072*\"line\" + 0.066*\"organization\" + 0.037*\"write\" + 0.032*\"article\" + '\n",
    "  '0.028*\"university\" + 0.027*\"nntp_post\" + 0.026*\"host\" + 0.016*\"reply\" + '\n",
    "  '0.014*\"get\" + 0.013*\"thank\"'),\n",
    " (2,\n",
    "  '0.017*\"patient\" + 0.011*\"study\" + 0.010*\"slave\" + 0.009*\"wing\" + '\n",
    "  '0.009*\"disease\" + 0.008*\"food\" + 0.008*\"eat\" + 0.008*\"pain\" + '\n",
    "  '0.007*\"treatment\" + 0.007*\"syndrome\"'),\n",
    " (3,\n",
    "  '0.013*\"key\" + 0.009*\"use\" + 0.009*\"may\" + 0.007*\"public\" + 0.007*\"system\" + '\n",
    "  '0.007*\"order\" + 0.007*\"government\" + 0.006*\"state\" + 0.006*\"provide\" + '\n",
    "  '0.006*\"law\"'),\n",
    "  \n",
    "  so on...\n",
    "\n",
    "\n",
    "\n",
    "How to interpret this?\n",
    "\n",
    "Topic 0 is a represented as _\n",
    "\n",
    "0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” \n",
    "+ 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” \n",
    "+ 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are:\n",
    "‘car’, ‘power’, ‘light’.. and so on\n",
    "and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "--> The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be?\n",
    "You may summarise it either are ‘cars’ or ‘automobiles’.\n",
    "\n",
    "Likewise, can you go through the remaining topic keywords and judge what the topic is?\n",
    "https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords.png\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -5.430642340072366\n",
      "\n",
      "Coherence Score:  0.5517435530091962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPerplexity:  -8.86067503009\\n\\nCoherence Score:  0.532947587081\\n\\n\\nThere you have a coherence score of 0.53.\\n'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Model perplexity and topic coherence provide a convenient measure to judge \n",
    "how good a given topic model is. In my experience, topic coherence score, \n",
    "in particular, has been more helpful.\n",
    "'''\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "'''\n",
    "Perplexity:  -8.86067503009\n",
    "\n",
    "Coherence Score:  0.532947587081\n",
    "\n",
    "\n",
    "There you have a coherence score of 0.53.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1782813583669772641507537229\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1782813583669772641507537229_data = {\"mdsDat\": {\"x\": [0.2207558286025079, -0.09714410474912215, -0.12361172385338569], \"y\": [0.008781131632837155, -0.11425042790139162, 0.10546929626855442], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [57.51356639011534, 21.71156497994125, 20.774868629943406]}, \"tinfo\": {\"Term\": [\"eye\", \"love\", \"sghome\", \"feelathome\", \"furniturestyle\", \"sginterior\", \"sghomedecor\", \"interiorsg\", \"instahome\", \"homestyling\", \"homestyle\", \"homefurniture\", \"teakfurniture\", \"homeinspo\", \"homeinspiration\", \"sgreno\", \"look\", \"cantik\", \"sparkle\", \"year\", \"think\", \"sofa\", \"heart_suit\", \"cover\", \"red_heart\", \"interior\", \"come\", \"hugging_face\", \"thank\", \"much\", \"sghome\", \"furniturestyle\", \"homefurniture\", \"homeinspiration\", \"homeinspo\", \"homestyle\", \"homestyling\", \"instahome\", \"interiorsg\", \"sghomedecor\", \"sginterior\", \"sgreno\", \"teakfurniture\", \"feelathome\", \"chair\", \"koniska\", \"promotion\", \"response\", \"brp\", \"place\", \"teak\", \"rachtjy\", \"https\", \"detail\", \"www\", \"feel\", \"table\", \"com\", \"cashconverter\", \"charm\", \"sale\", \"product\", \"item\", \"website\", \"love\", \"look\", \"year\", \"think\", \"sofa\", \"heart_suit\", \"cover\", \"thank\", \"machine\", \"guy\", \"cushion\", \"set\", \"hope\", \"purchase\", \"serve\", \"family\", \"raising_hand\", \"idea\", \"watch\", \"zhong\", \"jarrodstone\", \"get\", \"post\", \"name\", \"especially\", \"frame\", \"kid\", \"kor\", \"pet\", \"round\", \"come\", \"christma\", \"product\", \"many\", \"new\", \"eye\", \"cantik\", \"sparkle\", \"red_heart\", \"interior\", \"pm\", \"want\", \"cute\", \"rumah\", \"awww\", \"coffee\", \"pleasure\", \"join\", \"omg\", \"planter\", \"start\", \"fern\", \"keep\", \"struggle\", \"monkey\", \"also\", \"know\", \"langsirnya\", \"rumahnya\", \"nest\", \"langsir\", \"yet\", \"blossom\", \"re\", \"designer\", \"hugging_face\", \"much\", \"furniture\", \"great\"], \"Freq\": [25.0, 6.0, 20.0, 20.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 4.0, 4.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 20.429661860294505, 19.588353367552173, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 19.58834968237075, 20.14728483383146, 3.0906375141073865, 1.9969353219231585, 1.92053886232033, 1.9205329890624387, 1.920513641859973, 1.920506501820968, 1.9204866939708245, 1.9203316860272603, 1.6655825798470916, 1.665525689858889, 1.6655206227344337, 1.6655085307328927, 3.7441229046654403, 1.410517814845742, 1.0792027307173315, 1.0792027307173315, 2.7625547947337736, 3.090732177205165, 1.9205959826323715, 1.6657234228745648, 5.823471597629984, 4.222839845251632, 3.478447547416126, 2.735032719274909, 2.509506758770753, 2.2840833969358076, 2.2165995570028847, 1.7641239490372438, 1.6981514396706028, 1.4727117316070035, 1.4727120793991024, 1.4727006022598348, 1.4726905162889634, 1.47135673358923, 1.4713555163168834, 1.4713551685247843, 1.4706828863973829, 1.4426791882726164, 1.2472648938053743, 1.2472578510153691, 1.2472498517970918, 1.2472187244042297, 1.2471989872026104, 1.2471735114313574, 0.9542493547986618, 0.9542493547986618, 0.9542493547986618, 0.9542493547986618, 0.9542493547986618, 0.9542493547986618, 1.9763127781282641, 1.2472811530860033, 1.4725811356738223, 0.9542996107569699, 0.9542653532352167, 25.361666198265517, 3.9668413985100854, 3.5432739274182805, 2.082227772962733, 2.0822244450887752, 1.594393564756941, 1.3834286525983148, 1.3834278206298254, 1.3834262398896955, 1.3834258239054509, 1.3834235775905295, 1.1716473297610317, 0.8963915534022254, 0.8963915534022254, 0.8963915534022254, 0.8963915534022254, 0.8963886415125125, 0.8963886415125125, 0.8963886415125125, 0.8963858960164974, 0.8963838992921228, 0.8963838992921228, 0.8963838992921228, 0.8963838992921228, 0.8963749140324372, 0.8963749140324372, 0.8963748308355883, 0.896373749276552, 0.8963706709931413, 0.8963185065688553, 1.8704776489438026, 1.7778537650494006, 1.3835629323125054, 1.1717539881213739], \"Total\": [25.0, 6.0, 20.0, 20.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 4.0, 4.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 20.837953079478794, 19.996620631510883, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 19.996621414774797, 20.579792515765526, 3.500862847901482, 2.4049834233511507, 2.328577546966063, 2.3285760153835775, 2.3285711558844904, 2.328569274561818, 2.32856403948749, 2.3285274814938153, 2.073616937822121, 2.0736071683653647, 2.073606504277091, 2.073604253886368, 4.680450274237929, 1.8186147286778425, 1.4872372758852712, 1.4872372758852712, 3.9138480509056235, 4.763200107018463, 3.072443535297905, 2.560634954666384, 6.280039089127014, 4.658465959084853, 3.914534085754655, 3.170663659568042, 2.9452271924059907, 2.719766271316232, 2.652208166551453, 2.201224092468551, 2.13374533109751, 1.9083078177335164, 1.9083085830971553, 1.9083066434900513, 1.9083072292135783, 1.9082251979508582, 1.9082249276445475, 1.9082248886112503, 1.908505087058984, 1.9064820154976587, 1.682860311182505, 1.6828597308158433, 1.6828589542871542, 1.6828660430800706, 1.6828532172439266, 1.6828727357294664, 1.3898407682734684, 1.3898407682734684, 1.3898407682734684, 1.3898407682734684, 1.3898407682734684, 1.3898407682734684, 2.912859419791077, 2.1698574383854217, 4.763200107018463, 2.2310835256646495, 2.2310935689160076, 25.82462856963325, 4.415211192681504, 3.991650682870459, 2.5305815931067936, 2.5305845526276256, 2.0437064122772455, 1.8317804028452027, 1.831780690976343, 1.8317796614258783, 1.8317805552740158, 1.8317821291558454, 1.6199992988194445, 1.344746490628538, 1.344746490628538, 1.344746490628538, 1.344746490628538, 1.3447463354369664, 1.3447463354369664, 1.3447463354369664, 1.3447456316242292, 1.34474595887592, 1.34474595887592, 1.3447459732711602, 1.3447459732711602, 1.3447452857432078, 1.344745336270694, 1.344745253073845, 1.3447453833095961, 1.344748911249364, 1.3447631694407125, 2.8372263860283273, 2.923967864072118, 2.350196590553828, 2.138425080573366], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.1868, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2288, -3.2007, -5.0754, -5.5122, -5.5512, -5.5512, -5.5512, -5.5512, -5.5512, -5.5513, -5.6936, -5.6936, -5.6936, -5.6937, -4.8836, -5.8598, -6.1276, -6.1276, -5.1876, -5.0754, -5.5511, -5.6935, -3.4677, -3.7891, -3.983, -4.2235, -4.3095, -4.4036, -4.4336, -4.662, -4.7001, -4.8425, -4.8425, -4.8425, -4.8425, -4.8434, -4.8434, -4.8434, -4.8439, -4.8631, -5.0087, -5.0087, -5.0087, -5.0087, -5.0087, -5.0087, -5.2764, -5.2764, -5.2764, -5.2764, -5.2764, -5.2764, -4.5484, -5.0086, -4.8426, -5.2764, -5.2764, -1.9523, -3.8075, -3.9205, -4.4521, -4.4521, -4.719, -4.8609, -4.8609, -4.8609, -4.8609, -4.8609, -5.0271, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.2949, -5.295, -4.5593, -4.6101, -4.8608, -5.027], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5334, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5325, 0.5319, 0.4285, 0.3672, 0.3605, 0.3605, 0.3605, 0.3605, 0.3605, 0.3604, 0.334, 0.334, 0.334, 0.334, 0.3299, 0.299, 0.2325, 0.2325, 0.2048, 0.1206, 0.0833, 0.1232, 1.4518, 1.4291, 1.4092, 1.3795, 1.3672, 1.3527, 1.3479, 1.306, 1.299, 1.2682, 1.2682, 1.2682, 1.2682, 1.2673, 1.2673, 1.2673, 1.2667, 1.2486, 1.2278, 1.2278, 1.2278, 1.2277, 1.2277, 1.2277, 1.1513, 1.1513, 1.1513, 1.1513, 1.1513, 1.1513, 1.1394, 0.9736, 0.3534, 0.6781, 0.678, 1.5533, 1.4643, 1.4523, 1.3764, 1.3764, 1.3232, 1.2907, 1.2907, 1.2907, 1.2907, 1.2907, 1.2474, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1658, 1.1657, 1.1548, 1.0739, 1.0416, 0.9699]}, \"token.table\": {\"Topic\": [3, 3, 3, 1, 3, 1, 1, 1, 2, 3, 3, 1, 2, 3, 2, 2, 3, 3, 1, 2, 3, 2, 1, 1, 3, 2, 2, 3, 1, 2, 2, 3, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 3, 2, 1, 3, 1, 1, 2, 2, 3, 3, 2, 3, 1, 2, 3, 3, 2, 2, 2, 1, 2, 3, 1, 3, 2, 3, 1, 2, 3, 2, 1, 3, 3, 3, 2, 1, 2, 1, 2, 1, 2, 3, 3, 1, 2, 3, 3, 1, 2, 2, 2, 1, 1, 1, 1, 2, 3, 3, 3, 1, 2, 1, 1, 2, 2, 3, 2, 1, 3, 1, 2, 3, 2], \"Freq\": [0.7436348801790823, 0.5459169206272146, 0.7436351984632718, 0.8588958061023112, 0.9059589282230162, 0.6723876655154132, 0.8569315995336082, 0.6723876655154132, 0.4608597700059476, 0.4608597700059476, 0.5459164515710381, 0.5498690757481182, 0.6866105471521343, 0.34330527357606716, 0.7540886214073121, 0.5240242636109803, 0.545916880184493, 0.7436253629818702, 0.964502838585676, 0.7195068836858566, 0.9680681343621368, 0.5240472472444118, 0.9645041942075407, 0.9718270961516563, 0.74363467194358, 0.7195068836858566, 0.4254963197629132, 0.4254963197629132, 1.0001689969796093, 0.5942243615360775, 0.46763387180806665, 0.46763387180806665, 0.5240244737810134, 0.7353573066527144, 1.000168957803177, 1.000168957803177, 1.000168957803177, 1.000168957803177, 1.000168957803177, 0.5240246353896088, 0.9644982945116953, 0.35245689414296033, 0.7049137882859207, 0.5245263222370157, 1.000168957803177, 0.7903312291712622, 1.000168957803177, 0.650947682853374, 0.325473841426687, 0.5942268646177731, 0.7436345861238108, 0.74363467194358, 0.7195068836858566, 0.7436348801790823, 0.8316065635135069, 0.7195068836858566, 0.7436352244754708, 0.7436348722186178, 0.858651761144519, 0.9554080659128601, 0.9373189812540951, 0.4482127130144514, 0.4482127130144514, 0.7436350611469666, 0.3420010227497273, 0.6840020454994546, 0.5942219983536278, 0.7436352524168356, 0.44821069538820685, 0.44821069538820685, 0.7436345861238108, 0.7195068836858566, 0.8588965000306263, 0.7436345861238108, 0.6172842177948709, 0.9786141433942341, 0.5942288904065789, 0.6298286724464023, 0.20994289081546744, 0.8588934487519339, 0.5240471622918756, 0.858911915747262, 0.5239703088981571, 0.743633247541306, 0.7903321534654021, 0.8588940136749401, 0.7195068836858566, 0.5459171870166898, 0.7436348722186178, 0.7665090624317495, 0.2555030208105832, 0.5240472365248725, 0.5240247962304038, 0.9597871692923615, 1.000168957803177, 1.000168957803177, 1.000168957803177, 1.0185971417536943, 1.0020916953393169, 0.7436345861238108, 0.74363467194358, 0.8546186297537965, 0.21365465743844914, 0.8588984310005038, 1.000168957803177, 0.9085853670432577, 0.9461741521990091, 0.5459169660548587, 0.5942263854908577, 0.781056275263013, 0.3905281376315065, 0.9645031474750548, 0.7663747292218689, 0.7436352704827776, 0.5942265904212969], \"Term\": [\"also\", \"awww\", \"blossom\", \"brp\", \"cantik\", \"cashconverter\", \"chair\", \"charm\", \"christma\", \"christma\", \"coffee\", \"com\", \"come\", \"come\", \"cover\", \"cushion\", \"cute\", \"designer\", \"detail\", \"especially\", \"eye\", \"family\", \"feel\", \"feelathome\", \"fern\", \"frame\", \"furniture\", \"furniture\", \"furniturestyle\", \"get\", \"great\", \"great\", \"guy\", \"heart_suit\", \"homefurniture\", \"homeinspiration\", \"homeinspo\", \"homestyle\", \"homestyling\", \"hope\", \"https\", \"hugging_face\", \"hugging_face\", \"idea\", \"instahome\", \"interior\", \"interiorsg\", \"item\", \"item\", \"jarrodstone\", \"join\", \"keep\", \"kid\", \"know\", \"koniska\", \"kor\", \"langsir\", \"langsirnya\", \"look\", \"love\", \"machine\", \"many\", \"many\", \"monkey\", \"much\", \"much\", \"name\", \"nest\", \"new\", \"new\", \"omg\", \"pet\", \"place\", \"planter\", \"pleasure\", \"pm\", \"post\", \"product\", \"product\", \"promotion\", \"purchase\", \"rachtjy\", \"raising_hand\", \"re\", \"red_heart\", \"response\", \"round\", \"rumah\", \"rumahnya\", \"sale\", \"sale\", \"serve\", \"set\", \"sghome\", \"sghomedecor\", \"sginterior\", \"sgreno\", \"sofa\", \"sparkle\", \"start\", \"struggle\", \"table\", \"table\", \"teak\", \"teakfurniture\", \"thank\", \"think\", \"want\", \"watch\", \"website\", \"website\", \"www\", \"year\", \"yet\", \"zhong\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1782813583669772641507537229\", ldavis_el1782813583669772641507537229_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1782813583669772641507537229\", ldavis_el1782813583669772641507537229_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1782813583669772641507537229\", ldavis_el1782813583669772641507537229_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.220756  0.008781       1        1  57.513566\n",
       "2     -0.097144 -0.114250       2        1  21.711565\n",
       "0     -0.123612  0.105469       3        1  20.774869, topic_info=               Term       Freq      Total Category  logprob  loglift\n",
       "34              eye  25.000000  25.000000  Default  30.0000  30.0000\n",
       "44             love   6.000000   6.000000  Default  29.0000  29.0000\n",
       "230          sghome  20.000000  20.000000  Default  28.0000  28.0000\n",
       "86       feelathome  20.000000  20.000000  Default  27.0000  27.0000\n",
       "284  furniturestyle  19.000000  19.000000  Default  26.0000  26.0000\n",
       "..              ...        ...        ...      ...      ...      ...\n",
       "358        designer   0.896319   1.344763   Topic3  -5.2950   1.1657\n",
       "50     hugging_face   1.870478   2.837226   Topic3  -4.5593   1.1548\n",
       "8              much   1.777854   2.923968   Topic3  -4.6101   1.0739\n",
       "142       furniture   1.383563   2.350197   Topic3  -4.8608   1.0416\n",
       "48            great   1.171754   2.138425   Topic3  -5.0270   0.9699\n",
       "\n",
       "[133 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "276       3  0.743635     also\n",
       "95        3  0.545917     awww\n",
       "204       3  0.743635  blossom\n",
       "327       1  0.858896      brp\n",
       "155       3  0.905959   cantik\n",
       "...     ...       ...      ...\n",
       "112       3  0.390528  website\n",
       "113       1  0.964503      www\n",
       "72        2  0.766375     year\n",
       "232       3  0.743635      yet\n",
       "38        2  0.594227    zhong\n",
       "\n",
       "[115 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now that the LDA model is built, the next step is to examine the produced \n",
    "topics and the associated keywords. There is no better tool than pyLDAvis package’s \n",
    "interactive chart and is designed to work well with jupyter notebooks.\n",
    "'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSo how to infer pyLDAvis’s output?\\n\\nEach bubble on the left-hand side plot represents a topic. \\nThe larger the bubble, the more prevalent is that topic.\\n\\nA good topic model will have fairly big, non-overlapping bubbles scattered \\nthroughout the chart instead of being clustered in one quadrant.\\n\\nA model with too many topics, will typically have many overlaps, \\nsmall sized bubbles clustered in one region of the chart.\\n\\nAlright, if you move the cursor over one of the bubbles, \\nthe words and bars on the right-hand side will update. \\nThese words are the salient keywords that form the selected topic.\\n\\nWe have successfully built a good looking topic model.\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So how to infer pyLDAvis’s output?\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. \n",
    "The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered \n",
    "throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, \n",
    "small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, \n",
    "the words and bars on the right-hand side will update. \n",
    "These words are the salient keywords that form the selected topic.\n",
    "\n",
    "We have successfully built a good looking topic model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'scanteaksg_lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
