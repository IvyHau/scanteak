{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yong\n",
      "[nltk_data]     Jie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will need the stopwords from NLTK and spacy‚Äôs en model for text pre-processing.\n",
    "Later, we will be using the spacy model for lemmatization.\n",
    "\n",
    "Lemmatization is nothing but converting a word to its root word.\n",
    "For example: the lemma of the word ‚Äòmachines‚Äô is ‚Äòmachine‚Äô.\n",
    "\n",
    "Likewise, ‚Äòwalking‚Äô ‚Äì> ‚Äòwalk‚Äô, ‚Äòmice‚Äô ‚Äì> ‚Äòmouse‚Äô and so on.\n",
    "\n",
    "'''\n",
    "\n",
    "# Run in python console\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# nltk.download('stopwords') # You already did this in a previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim (Topic Modeling Pacakge)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-03 07:36:59</td>\n",
       "      <td>michelleyuan</td>\n",
       "      <td>üíõ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-11 01:59:34</td>\n",
       "      <td>mdflmvmt</td>\n",
       "      <td>Hi, come meditate with us! üßòüèª‚Äç‚ôÄÔ∏è..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 17:58:34</td>\n",
       "      <td>shuchsmooch</td>\n",
       "      <td>When are u going @theprojectweekend !!!!! Can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-30 18:15:10</td>\n",
       "      <td>theprojectweekend</td>\n",
       "      <td>@shuchsmooch I'm going down again next week! I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-30 17:24:30</td>\n",
       "      <td>shuchsmooch</td>\n",
       "      <td>Omg I love this !!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date           username  \\\n",
       "0  2017-03-03 07:36:59       michelleyuan   \n",
       "1  2018-10-11 01:59:34           mdflmvmt   \n",
       "2  2017-05-01 17:58:34        shuchsmooch   \n",
       "3  2017-04-30 18:15:10  theprojectweekend   \n",
       "4  2017-04-30 17:24:30        shuchsmooch   \n",
       "\n",
       "                                                text  \n",
       "0                                                  üíõ  \n",
       "1                 Hi, come meditate with us! üßòüèª‚Äç‚ôÄÔ∏è..  \n",
       "2  When are u going @theprojectweekend !!!!! Can ...  \n",
       "3  @shuchsmooch I'm going down again next week! I...  \n",
       "4                              Omg I love this !!!!!  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will be using the 20-Newsgroups dataset for this exercise.\n",
    "This version of the dataset contains about 11k newsgroups posts from \n",
    "20 different topics. This is available as newsgroups.json.\n",
    "'''\n",
    "# Import Dataset\n",
    "df = pd.read_csv('./greyandsanders.csv', encoding='utf-8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Yong\n",
      "[nltk_data]     Jie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Hi, come meditate with us! ..', 'When are u going @theprojectweekend !!!!! Can I tag along??????? I wanna check out ', \"@shuchsmooch I'm going down again next week! I've got my eye on a couple of chairs and a side table. Can customise the pieces (type of wood, colour, size)! Check it out.\", 'Omg I love this !!!!!', '#greyandsanders #pickjunction #furniture #design #home #interiors #interiordesign #decor #sgdesign #singapore #sgig #igsg #instasg #homedecor #home', 'The best of the best', '', 'Hi, how much is this for a 2m table?', '', 'Hi @mynameisiska! Thanks for your interest (: this table is indeed an Oak piece. We do have some Oak slabs on display in our studio. Feel free to drop by to take a look!', 'Hi there! What wood is this? Oak I reckon? Any of such slabs in stock? ', 'Looks like the one at Noshery too!', 'Thank you for for the lovely work !! We love the end product too !!', 'Stunning wall!', 'Wow! That wall!! ', '@jrwatts123 wallpaper ', 'Love this wall xo', '@delyanakumaat', '@ho.senkee']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As you can see there are many emails, newline and extra spaces that is quite distracting.\n",
    "Let‚Äôs get rid of them using regular expressions.\n",
    "'''\n",
    "# Convert to list\n",
    "data = df.text.values.tolist()\n",
    "new_data = []\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub(r'[^\\x00-\\x7F]+', '', sent) for sent in data]\n",
    "\n",
    "# # Remove new line characters\n",
    "# data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# # Remove distracting single quotes\n",
    "# data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "print(data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[[], ['come', 'with'], ['are', 'can', 'along', 'wanna', 'out'], ['shuchsmooch', 'down', 'next', 've', 'my', 'on', 'of', 'chairs', 'side', 'can', 'customise', 'pieces', 'of', 'colour', 'check', 'out'], ['omg', 'this'], ['greyandsanders', 'pickjunction', 'design', 'interiors', 'interiordesign', 'decor', 'sgdesign', 'singapore', 'sgig', 'igsg', 'instasg', 'homedecor'], ['the', 'best'], [], ['how', 'is', 'for'], [], ['mynameisiska', 'for', 'interest', 'table', 'indeed', 'oak', 'we', 'have', 'oak', 'slabs', 'display', 'our', 'feel', 'by', 'to', 'take'], ['there', 'wood', 'this', 'reckon', 'of', 'slabs', 'stock'], ['looks', 'the', 'at', 'noshery'], ['you', 'for', 'lovely', 'we', 'the', 'product'], ['wall'], ['that'], ['jrwatts'], ['this', 'xo'], ['delyanakumaat'], ['senkee'], [], [], ['decor', 'inspirations', 'your', 'is', 'let', 'know', 'you', 'any', 'plans', 'mind'], ['huat', 'let', 'know', 'you', 'looking', 'really', 'tiles'], ['id', 'to', 'where', 'the', 'walnut', 'slabs', 'sourced'], [], ['wait', 'see', 'for', 'there', 'nearly', 'up', 'for', 'table'], ['may', 'whats', 'length', 'the'], ['how', 'are', 'legs', 'would', 'it', 'to', 'our'], ['may', 'how', 'is', 'dining'], ['true', 'so', 'said'], ['niceeee', 'greetings', 'other', 'of', 'world'], ['scandinavian', 'minimalist', 'minimalistdeco', 'mujiinspired', 'ilovesg', 'singaporebto', 'singaporeflats', 'neat', 'livingroom', 'visionblinds'], ['price', 'including', 'size'], ['the', 'still'], ['have', 'piece'], ['is', 'dimension', 'tag'], [], [], [], [], [], ['homendecor', 'decorhome', 'decorhomestyle', 'scandanaviadeco', 'singaporedecor', 'decorsingapore', 'homedecoideas', 'furnituresg', 'furnitureideas', 'bto', 'btosg', 'landedhome', 'landedhouse', 'homedecosg', 'homedecortips', 'homedecorsg', 'nordichome', 'nordichomedecor', 'furnituresg', 'speckledspace', 'greyandsanders', 'walnutslabtable'], ['homendecor', 'decorhome', 'decorhomestyle', 'scandanaviadeco', 'singaporedecor', 'decorsingapore', 'homedecoideas', 'furnituresg', 'furnitureideas', 'bto', 'btosg', 'landedhome', 'landedhouse', 'homedecosg', 'homedecortips', 'homedecorsg', 'posterdesign', 'nordichome', 'nordichomedecor', 'furnituresg', 'ootdsg', 'lookbooksg', 'sgootd', 'speckledspace', 'greyandsanders', 'woodslabstable'], ['weekend'], [], [], ['homendecor', 'decorhome', 'decorhomestyle', 'scandanaviadeco', 'singaporedecor', 'decorsingapore', 'homedecoideas', 'furnituresg', 'furnitureideas', 'bto', 'btosg', 'landedhome', 'landedhouse', 'homedecosg', 'homedecortips', 'homedecorsg', 'posterdesign', 'nordichome', 'nordichomedecor', 'furnituresg', 'sgootd', 'speckledspace', 'greyandsanders', 'dontsaybojio'], ['much', 'the', 'seat'], ['rosheila'], [], ['is', 'question', 'is', 'answer'], ['life'], ['leather'], ['photo', 'us', 'if', 'know', 'who', 'wants', 'make', 'car', 'bike'], ['much', 'this', 'table'], [], ['how', 'is', 'sofa', 'pls'], [], ['truly', 'fab', 'everything', 'you', 'featured', 'get', 'touch', 'us', 'to', 'get', 'info'], ['is', 'over', 'not', 'any', 'deals', 'last', 'saturday'], ['just', 'last', 'sunday', 'price'], ['know', 'price', 'seater', 'all', 'covers', 'greyandsanders'], ['this'], ['guys', 'the', 'thank', 'for'], ['awsome', 'love'], [], ['perfectly', 'fab', 'in', 'featured', 'our', 'next', 'with', 'for', 'info'], [], [], [], ['soo', 'perfect', 'in', 'featured', 'our', 'page', 'next', 'us', 'more'], [], ['for', 'great', 'sales', 'love', 'dining', 'even', 'now'], ['my', 'table', 'so', 'we', 'it', 'thanks', 'samantha'], [], ['could', 'tell', 'the', 'dimensions', 'this', 'please'], ['post', 'greyandsanders', 'you', 'blog'], [], ['niceeeeee']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After removing the emails and extra spaces, the text still looks messy.\n",
    "It is not ready for the LDA to consume. You need to break down each sentence\n",
    "into a list of words through tokenization, while clearing up all the messy \n",
    "text in the process.\n",
    "\n",
    "Let‚Äôs tokenize each sentence into a list of words, removing punctuations and \n",
    "unnecessary characters altogether.\n",
    "\n",
    "Gensim‚Äôs simple_preprocess() is great for this.\n",
    "Additionally I have set deacc=True to remove the punctuations.\n",
    "'''\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[0])\n",
    "for list_of_words in data_words:\n",
    "    for word in list_of_words:\n",
    "        if word.lower() in words or not word.isalpha():\n",
    "            list_of_words.remove(word)\n",
    "    \n",
    "print(data_words[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['come', 'with'], ['are', 'can', 'along', 'wanna', 'out'], ['shuchsmooch', 'down', 'next', 've', 'my', 'on', 'of', 'chairs', 'side', 'can', 'customise', 'pieces', 'of', 'colour', 'check', 'out'], ['omg', 'this'], ['greyandsanders', 'pickjunction', 'design', 'interiors', 'interiordesign', 'decor', 'sgdesign', 'singapore', 'sgig', 'igsg', 'instasg', 'homedecor'], ['the', 'best'], ['how', 'is', 'for'], ['mynameisiska', 'for', 'interest', 'table', 'indeed', 'oak', 'we', 'have', 'oak', 'slabs', 'display', 'our', 'feel', 'by', 'to', 'take'], ['there', 'wood', 'this', 'reckon', 'of', 'slabs', 'stock'], ['looks', 'the', 'at', 'noshery'], ['you', 'for', 'lovely', 'we', 'the', 'product'], ['wall'], ['that'], ['jrwatts'], ['this', 'xo'], ['delyanakumaat'], ['senkee'], ['decor', 'inspirations', 'your', 'is', 'let', 'know', 'you', 'any', 'plans', 'mind'], ['huat', 'let', 'know', 'you', 'looking', 'really', 'tiles'], ['id', 'to', 'where', 'the', 'walnut', 'slabs', 'sourced']]\n"
     ]
    }
   ],
   "source": [
    "data_words = [x for x in data_words if x]\n",
    "\n",
    "print(data_words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['come', 'with']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Bigrams are two words frequently occurring together in the document.\n",
    "Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‚Äòfront_bumper‚Äô, ‚Äòoil_leak‚Äô, ‚Äòmaryland_college_park‚Äô etc.\n",
    "\n",
    "Gensim‚Äôs Phrases model can build and implement the bigrams, trigrams, \n",
    "quadgrams and more.\n",
    "\n",
    "The two important arguments to Phrases are min_count and threshold.\n",
    "The higher the values of these param, the harder it is for words to be combined to bigrams.\n",
    "'''\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The bigrams model is ready. Let‚Äôs define the functions to remove the stopwords, \n",
    "make bigrams and lemmatization and call them sequentially.\n",
    "'''\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['come']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let‚Äôs call the functions in order.\n",
    "'''\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus.\n",
    "Let‚Äôs create them.\n",
    "'''\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGensim creates a unique id for each word in the document.\\nThe produced corpus shown above is a mapping of (word_id, word_frequency).\\n\\nFor example, (0, 1) above implies, word id 0 occurs once in the first document.\\n\\nLikewise, word id 1 occurs twice and so on.\\n\\nThis is used as the input by the LDA model.\\n\\nIf you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\\n\\nid2word[0]\\n'addition'\\n\\nOr, you can see a human-readable form of the corpus itself.\\n\\n# Human readable format of corpus (term-frequency)\\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\\n\\n[[('addition', 1),\\n  ('anyone', 2),\\n  ('body', 1),\\n  ('bricklin', 1),\\n  ('bring', 1),\\n  ('call', 1),\\n  ('car', 5),\\n  ('could', 1),\\n  ('day', 1),\\n  ('door', 2),\\n  ('early', 1),\\n  ('engine', 1),\\n  ('enlighten', 1),\\n  ('front_bumper', 1),\\n  ('maryland_college', 1),\\n  (..truncated..)]]\\n\\n\\nAlright, without digressing further let‚Äôs jump back on track with the next step:\\n\\nBuilding the topic model.\\n\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Gensim creates a unique id for each word in the document.\n",
    "The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document.\n",
    "\n",
    "Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\n",
    "\n",
    "id2word[0]\n",
    "'addition'\n",
    "\n",
    "Or, you can see a human-readable form of the corpus itself.\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "[[('addition', 1),\n",
    "  ('anyone', 2),\n",
    "  ('body', 1),\n",
    "  ('bricklin', 1),\n",
    "  ('bring', 1),\n",
    "  ('call', 1),\n",
    "  ('car', 5),\n",
    "  ('could', 1),\n",
    "  ('day', 1),\n",
    "  ('door', 2),\n",
    "  ('early', 1),\n",
    "  ('engine', 1),\n",
    "  ('enlighten', 1),\n",
    "  ('front_bumper', 1),\n",
    "  ('maryland_college', 1),\n",
    "  (..truncated..)]]\n",
    "\n",
    "\n",
    "Alright, without digressing further let‚Äôs jump back on track with the next step:\n",
    "\n",
    "Building the topic model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We have everything required to train the LDA model.\n",
    "In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics.\n",
    "According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk.\n",
    "\n",
    "update_every determines how often the model parameters should be updated and \n",
    "passes is the total number of training passes.\n",
    "'''\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.025*\"much\" + 0.019*\"greyandsander\" + 0.017*\"dm\" + 0.012*\"time\" + '\n",
      "  '0.010*\"look\" + 0.010*\"decor\" + 0.010*\"interior\" + 0.009*\"get\" + '\n",
      "  '0.009*\"wood\" + 0.009*\"sofas\"'),\n",
      " (1,\n",
      "  '0.023*\"italian\" + 0.023*\"price\" + 0.023*\"sofa\" + 0.021*\"grain\" + '\n",
      "  '0.020*\"table\" + 0.019*\"jethro\" + 0.018*\"know\" + 0.016*\"leather\" + '\n",
      "  '0.015*\"chair\" + 0.012*\"love\"'),\n",
      " (2,\n",
      "  '0.068*\"homedecor\" + 0.064*\"homeinterior\" + 0.061*\"homegoal\" + '\n",
      "  '0.061*\"sghomedecor\" + 0.061*\"homeinspo\" + 0.058*\"interiordesign\" + '\n",
      "  '0.041*\"singapore\" + 0.040*\"instasg\" + 0.039*\"furnituredesign\" + '\n",
      "  '0.037*\"greyandsander\"')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe output will look like this...\\n\\n\\n[(0,\\n  \\'0.016*\"car\" + 0.014*\"power\" + 0.010*\"light\" + 0.009*\"drive\" + 0.007*\"mount\" \\'\\n  \\'+ 0.007*\"controller\" + 0.007*\"cool\" + 0.007*\"engine\" + 0.007*\"back\" + \\'\\n  \\'0.006*\"turn\"\\'),\\n (1,\\n  \\'0.072*\"line\" + 0.066*\"organization\" + 0.037*\"write\" + 0.032*\"article\" + \\'\\n  \\'0.028*\"university\" + 0.027*\"nntp_post\" + 0.026*\"host\" + 0.016*\"reply\" + \\'\\n  \\'0.014*\"get\" + 0.013*\"thank\"\\'),\\n (2,\\n  \\'0.017*\"patient\" + 0.011*\"study\" + 0.010*\"slave\" + 0.009*\"wing\" + \\'\\n  \\'0.009*\"disease\" + 0.008*\"food\" + 0.008*\"eat\" + 0.008*\"pain\" + \\'\\n  \\'0.007*\"treatment\" + 0.007*\"syndrome\"\\'),\\n (3,\\n  \\'0.013*\"key\" + 0.009*\"use\" + 0.009*\"may\" + 0.007*\"public\" + 0.007*\"system\" + \\'\\n  \\'0.007*\"order\" + 0.007*\"government\" + 0.006*\"state\" + 0.006*\"provide\" + \\'\\n  \\'0.006*\"law\"\\'),\\n  \\n  so on...\\n\\n\\n\\nHow to interpret this?\\n\\nTopic 0 is a represented as _\\n\\n0.016‚Äúcar‚Äù + 0.014‚Äúpower‚Äù + 0.010‚Äúlight‚Äù + 0.009‚Äúdrive‚Äù \\n+ 0.007‚Äúmount‚Äù + 0.007‚Äúcontroller‚Äù + 0.007‚Äúcool‚Äù + 0.007‚Äúengine‚Äù \\n+ 0.007‚Äúback‚Äù + ‚Äò0.006‚Äúturn‚Äù.\\n\\nIt means the top 10 keywords that contribute to this topic are:\\n‚Äòcar‚Äô, ‚Äòpower‚Äô, ‚Äòlight‚Äô.. and so on\\nand the weight of ‚Äòcar‚Äô on topic 0 is 0.016.\\n\\n--> The weights reflect how important a keyword is to that topic.\\n\\nLooking at these keywords, can you guess what this topic could be?\\nYou may summarise it either are ‚Äòcars‚Äô or ‚Äòautomobiles‚Äô.\\n\\nLikewise, can you go through the remaining topic keywords and judge what the topic is?\\nhttps://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords.png\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The above LDA model is built with 20 different topics where each topic is a \n",
    "combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of \n",
    "each keyword using lda_model.print_topics() as shown next.\n",
    "'''\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "'''\n",
    "The output will look like this...\n",
    "\n",
    "\n",
    "[(0,\n",
    "  '0.016*\"car\" + 0.014*\"power\" + 0.010*\"light\" + 0.009*\"drive\" + 0.007*\"mount\" '\n",
    "  '+ 0.007*\"controller\" + 0.007*\"cool\" + 0.007*\"engine\" + 0.007*\"back\" + '\n",
    "  '0.006*\"turn\"'),\n",
    " (1,\n",
    "  '0.072*\"line\" + 0.066*\"organization\" + 0.037*\"write\" + 0.032*\"article\" + '\n",
    "  '0.028*\"university\" + 0.027*\"nntp_post\" + 0.026*\"host\" + 0.016*\"reply\" + '\n",
    "  '0.014*\"get\" + 0.013*\"thank\"'),\n",
    " (2,\n",
    "  '0.017*\"patient\" + 0.011*\"study\" + 0.010*\"slave\" + 0.009*\"wing\" + '\n",
    "  '0.009*\"disease\" + 0.008*\"food\" + 0.008*\"eat\" + 0.008*\"pain\" + '\n",
    "  '0.007*\"treatment\" + 0.007*\"syndrome\"'),\n",
    " (3,\n",
    "  '0.013*\"key\" + 0.009*\"use\" + 0.009*\"may\" + 0.007*\"public\" + 0.007*\"system\" + '\n",
    "  '0.007*\"order\" + 0.007*\"government\" + 0.006*\"state\" + 0.006*\"provide\" + '\n",
    "  '0.006*\"law\"'),\n",
    "  \n",
    "  so on...\n",
    "\n",
    "\n",
    "\n",
    "How to interpret this?\n",
    "\n",
    "Topic 0 is a represented as _\n",
    "\n",
    "0.016‚Äúcar‚Äù + 0.014‚Äúpower‚Äù + 0.010‚Äúlight‚Äù + 0.009‚Äúdrive‚Äù \n",
    "+ 0.007‚Äúmount‚Äù + 0.007‚Äúcontroller‚Äù + 0.007‚Äúcool‚Äù + 0.007‚Äúengine‚Äù \n",
    "+ 0.007‚Äúback‚Äù + ‚Äò0.006‚Äúturn‚Äù.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are:\n",
    "‚Äòcar‚Äô, ‚Äòpower‚Äô, ‚Äòlight‚Äô.. and so on\n",
    "and the weight of ‚Äòcar‚Äô on topic 0 is 0.016.\n",
    "\n",
    "--> The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be?\n",
    "You may summarise it either are ‚Äòcars‚Äô or ‚Äòautomobiles‚Äô.\n",
    "\n",
    "Likewise, can you go through the remaining topic keywords and judge what the topic is?\n",
    "https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords.png\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -4.516973794531733\n",
      "\n",
      "Coherence Score:  0.7746888618420756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPerplexity:  -8.86067503009\\n\\nCoherence Score:  0.532947587081\\n\\n\\nThere you have a coherence score of 0.53.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Model perplexity and topic coherence provide a convenient measure to judge \n",
    "how good a given topic model is. In my experience, topic coherence score, \n",
    "in particular, has been more helpful.\n",
    "'''\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "'''\n",
    "Perplexity:  -8.86067503009\n",
    "\n",
    "Coherence Score:  0.532947587081\n",
    "\n",
    "\n",
    "There you have a coherence score of 0.53.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1323625588510501765258823071\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1323625588510501765258823071_data = {\"mdsDat\": {\"x\": [0.32803385135554003, -0.15903407996334745, -0.16899977139219258], \"y\": [0.002497001758973678, -0.12453665048045391, 0.1220396487214803], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [71.84575003507098, 15.430356038733631, 12.723893926195387]}, \"tinfo\": {\"Term\": [\"homedecor\", \"homeinterior\", \"homegoal\", \"sghomedecor\", \"homeinspo\", \"interiordesign\", \"much\", \"singapore\", \"instasg\", \"furnituredesign\", \"italian\", \"price\", \"sofa\", \"grain\", \"table\", \"jethro\", \"livingroom\", \"dm\", \"know\", \"solidwood\", \"luxurylifestyle\", \"livingroomdecor\", \"instagram\", \"leather\", \"sghome\", \"interiordeco\", \"chair\", \"igsg\", \"interiordecor\", \"design\", \"homedecor\", \"homeinterior\", \"homegoal\", \"sghomedecor\", \"homeinspo\", \"interiordesign\", \"singapore\", \"instasg\", \"furnituredesign\", \"solidwood\", \"luxurylifestyle\", \"livingroomdecor\", \"livingroom\", \"instagram\", \"sghome\", \"interiordeco\", \"interiordecor\", \"igsg\", \"design\", \"blackwalnut\", \"woodslabstable\", \"homedecoration\", \"homeinspiration\", \"interiorsg\", \"ig\", \"diningchair\", \"woodchair\", \"homedesign\", \"japanesemethod\", \"leathersofa\", \"greyandsander\", \"much\", \"dm\", \"time\", \"decor\", \"get\", \"wood\", \"furnituresg\", \"sofas\", \"dining\", \"fluidart\", \"artwork\", \"deliver\", \"schedule\", \"sgart\", \"info\", \"dimension\", \"awesome\", \"nice\", \"bench\", \"message\", \"plan\", \"well\", \"good\", \"beautiful\", \"say\", \"color\", \"minimalist\", \"send\", \"singaporedecor\", \"landedhouse\", \"style\", \"interior\", \"look\", \"greyandsander\", \"italian\", \"sofa\", \"grain\", \"price\", \"table\", \"jethro\", \"leather\", \"chair\", \"love\", \"size\", \"congratulation\", \"colour\", \"thank\", \"post\", \"want\", \"know\", \"livefastdiey\", \"sufianv\", \"congrat\", \"geowhee\", \"check\", \"natural\", \"buy\", \"s\", \"seater\", \"bring\", \"collection\", \"dark\", \"gorgeous\", \"ve\", \"pm\", \"let\", \"take\", \"look\", \"greyandsander\"], \"Freq\": [118.0, 111.0, 106.0, 106.0, 106.0, 100.0, 9.0, 71.0, 69.0, 68.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 57.0, 6.0, 7.0, 53.0, 53.0, 53.0, 53.0, 5.0, 52.0, 50.0, 5.0, 48.0, 48.0, 48.0, 118.16275581480993, 110.6839744874282, 106.43607629325481, 106.43607629325481, 105.8615455146917, 100.13269348921988, 71.28226490363157, 69.11702140683566, 67.7681399049169, 53.34049083128283, 53.12366617043863, 52.92519924046298, 57.00035716109513, 52.76981243142264, 51.99752513639154, 49.78214259171596, 47.89975023299715, 48.035177651624394, 48.181270960323545, 35.84422029874357, 39.803138423760075, 12.66041555179595, 12.66041555179595, 12.66041555179595, 7.814521136528404, 4.783788532580062, 4.783788532580062, 14.175421601465807, 3.6556268564765007, 3.800357059257817, 64.6650214119099, 9.312020059198217, 6.190402872041903, 4.283538506622873, 3.7308106388497237, 3.446201015311967, 3.2794511094701635, 3.2461510622481327, 3.249275748292407, 2.783174088167839, 2.282995977308519, 2.2829826269131464, 2.2829907758557764, 2.2829907758557764, 2.2810577426348457, 2.2458270895364967, 2.2599625575099522, 2.2682346011883636, 1.9634405669009534, 1.909030076959317, 1.7642665384789828, 1.764267232006015, 1.7642653248066762, 1.7642602967356915, 1.7642568291005298, 1.764260816880966, 1.764251454266029, 1.764075645163327, 1.9152534417841525, 1.7456668369982598, 1.7456666636165017, 2.7386680311570193, 3.6821458469893886, 3.789326635441143, 7.1822533314722286, 7.090655254194511, 6.93196047155039, 6.405604713796837, 6.999866477734089, 6.199119923381626, 5.720564467300269, 4.839630710860461, 4.526293511307877, 3.6770335166898835, 3.54964391701962, 3.2642286382771686, 2.4132404120400452, 2.395004766229587, 1.7281825801290873, 1.7043657812977833, 5.435401675076629, 1.6102573647013105, 1.6102573647013105, 1.6102496442754806, 1.6102427816747427, 1.6871156335474395, 1.4150984389787087, 1.4150719893716985, 1.4144662218857411, 1.4028604202713704, 1.238204611061398, 1.238204611061398, 1.238204611061398, 1.238203610265457, 1.238203610265457, 2.295326491309316, 1.604591572938032, 1.4152123867451256, 1.604561692030653, 2.7752142927297423], \"Total\": [118.0, 111.0, 106.0, 106.0, 106.0, 100.0, 9.0, 71.0, 69.0, 68.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 57.0, 6.0, 7.0, 53.0, 53.0, 53.0, 53.0, 5.0, 52.0, 50.0, 5.0, 48.0, 48.0, 48.0, 118.65360434144624, 111.17231934329313, 106.92173999949682, 106.92173999949682, 106.34734125652375, 100.62361413994473, 71.77791734410232, 69.6048098653491, 68.25610741718032, 53.826418591870116, 53.61069520690981, 53.41091612553645, 57.52439314205652, 53.25685358837568, 52.50124353310396, 50.2675967915774, 48.38560253904787, 48.52352414109916, 48.67659170707224, 36.32970000739617, 40.788672564367, 13.164400032800279, 13.164400032800279, 13.164400032800279, 8.29997546148933, 5.269255204918965, 5.269255204918965, 15.83036922043886, 4.141088902257465, 4.335633758972144, 74.62248903611187, 9.829748603696272, 6.730853065313361, 4.83177848761587, 4.259640905548874, 3.9753277649807783, 3.7951506808876903, 3.7579031204588054, 3.777828354597266, 3.2949287015748796, 2.794740728539133, 2.7947292619517903, 2.794741067206777, 2.794741067206777, 2.7949319779214785, 2.757576665584946, 2.7760771653934975, 2.7944050163777865, 2.4751866374125986, 2.4569684607525843, 2.2760113759078133, 2.2760122765347224, 2.2760108928418123, 2.2760061212461844, 2.276002407650626, 2.276011135293727, 2.2760110257822874, 2.2760312068494155, 2.4741656142345256, 2.2574149689107164, 2.2574150109837454, 3.748362618897645, 5.259616665075194, 5.66542436449666, 74.62248903611187, 7.607431931893042, 7.471868460029861, 6.922381871165188, 7.590614897755365, 6.75047075635257, 6.237329474269901, 5.3572685439697585, 5.104263808008395, 4.223475373862455, 4.105786189444957, 3.8107239682616783, 2.930001096843442, 2.9117907704266974, 2.2449433118329565, 2.2274261487696645, 7.1045424158382815, 2.1270185582998358, 2.1270185582998358, 2.127012349696737, 2.127006883445066, 2.2457302121737435, 1.9319286736994523, 1.931931554684261, 1.9320032843986772, 1.9321576862655354, 1.7549652237365703, 1.7549652237365703, 1.7549652237365703, 1.7549642698411014, 1.7549642698411014, 3.8308889108908044, 3.127408386639548, 2.950753535408478, 5.66542436449666, 74.62248903611187], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.6859, -2.7513, -2.7904, -2.7904, -2.7958, -2.8515, -3.1913, -3.2222, -3.2419, -3.4813, -3.4854, -3.4891, -3.4149, -3.492, -3.5068, -3.5503, -3.5889, -3.586, -3.583, -3.8788, -3.774, -4.9195, -4.9195, -4.9195, -5.402, -5.8927, -5.8927, -4.8065, -6.1617, -6.1229, -3.2888, -3.6885, -4.0968, -4.465, -4.6032, -4.6825, -4.7321, -4.7423, -4.7414, -4.8962, -5.0943, -5.0943, -5.0943, -5.0943, -5.0952, -5.1107, -5.1044, -5.1008, -5.2451, -5.2732, -5.3521, -5.3521, -5.3521, -5.3521, -5.3521, -5.3521, -5.3521, -5.3522, -5.2699, -5.3627, -5.3627, -4.9123, -4.6163, -4.5876, -3.9482, -3.7682, -3.7908, -3.8698, -3.781, -3.9025, -3.9829, -4.1501, -4.217, -4.4248, -4.4601, -4.5439, -4.846, -4.8536, -5.1799, -5.1937, -4.034, -5.2505, -5.2505, -5.2505, -5.2506, -5.2039, -5.3797, -5.3798, -5.3802, -5.3884, -5.5133, -5.5133, -5.5133, -5.5133, -5.5133, -4.8961, -5.2541, -5.3797, -5.2541, -4.7062], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3265, 0.3262, 0.3261, 0.3261, 0.3261, 0.3258, 0.3237, 0.3236, 0.3235, 0.3216, 0.3215, 0.3215, 0.3215, 0.3215, 0.321, 0.3209, 0.3206, 0.3205, 0.3204, 0.3172, 0.3062, 0.2916, 0.2916, 0.2916, 0.2704, 0.234, 0.234, 0.2202, 0.206, 0.1989, 0.1874, 1.8147, 1.7851, 1.7484, 1.7363, 1.726, 1.7228, 1.7224, 1.7181, 1.7, 1.6666, 1.6666, 1.6666, 1.6666, 1.6657, 1.6636, 1.6631, 1.6602, 1.6372, 1.6165, 1.6141, 1.6141, 1.6141, 1.6141, 1.6141, 1.6141, 1.6141, 1.614, 1.6128, 1.6117, 1.6117, 1.555, 1.5123, 1.4666, -0.472, 1.9913, 1.9867, 1.9841, 1.9807, 1.9765, 1.9752, 1.9601, 1.9415, 1.9231, 1.9161, 1.9069, 1.8677, 1.8663, 1.8001, 1.794, 1.7939, 1.7834, 1.7834, 1.7834, 1.7834, 1.7757, 1.7504, 1.7503, 1.7499, 1.7416, 1.7129, 1.7129, 1.7129, 1.7129, 1.7129, 1.5495, 1.3944, 1.3269, 0.8002, -1.23]}, \"token.table\": {\"Topic\": [2, 2, 2, 2, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 3, 2, 2, 3, 3, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 3, 2, 3, 2, 3, 1, 2, 3, 3, 1, 1, 2, 3, 3, 1, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 1, 1, 1, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 1, 1, 2], \"Freq\": [0.7156328261304405, 0.7157158637628255, 0.8787336925818432, 0.8140112630454311, 0.9909247803497125, 0.5698118609272826, 0.5176166813857087, 0.9795731937199623, 0.8905789258025388, 0.5698118609272826, 0.8787303652505727, 0.682593601126855, 0.9402860309133391, 0.7872519828216519, 0.5698118609272826, 0.9390462925617392, 0.7156298032285737, 0.9861002653771682, 0.7204410687613246, 0.910490111232479, 0.9489007090285532, 0.8914174684514027, 0.7156298899488398, 0.9962478461360975, 0.7983175467370025, 0.9402884473794669, 0.7546547548676167, 0.8787322588152521, 0.5698121706435324, 0.8667536856053376, 0.8710510844598699, 0.0938055014033706, 0.04020235774430169, 0.9944914918929443, 0.9875117717183722, 0.8843760878251886, 0.06316972055894204, 0.9913793022868768, 0.9875117717183722, 0.9967338980700428, 0.9984499797763415, 0.9638582712827077, 0.9892109208808324, 0.7252744864577513, 0.9951770791725529, 0.9913108035706282, 0.1901279244626668, 0.7605116978506672, 0.9946765549050033, 0.9920306347588276, 0.9938025070429549, 0.9875117717183722, 0.9201528272180165, 0.965929516224452, 0.9619501462526667, 0.14075501861607334, 0.7037750930803667, 0.885969124094923, 0.9333114364087821, 0.9225871515836445, 0.3197535711268321, 0.6395071422536642, 0.9402832862909462, 0.9908839865418219, 0.9923065141857773, 0.7060371373178462, 0.3530185686589231, 0.947087326412399, 0.9886087056966368, 0.8787302300728954, 0.8787225737420752, 0.9155880137784742, 0.5176174532805597, 0.8080198760650517, 0.8787298823559261, 0.2610360214719638, 0.5220720429439276, 0.8908910926427961, 0.9221914290593222, 0.5175974637699662, 0.8787303229700119, 0.7156298032285737, 0.517556101713828, 0.8083533246495198, 0.7155809213959298, 0.9904527302712762, 0.9913793022868768, 0.9891621633381615, 0.8859691406073522, 0.97423485184959, 0.9368473277394962, 0.7941070155686875, 0.9846465989473255, 0.8003494605551982, 0.2667831535183994, 0.9402832862909462, 0.8888269006059563, 0.3388964845759536, 0.3388964845759536, 0.6868625384463725, 0.8278525206095919, 0.5698121706435324, 0.8978973337027201, 0.8787304165767033, 0.7904824478005433, 0.9489007090285532, 0.9806644218901112, 0.02451661054725278], \"Term\": [\"artwork\", \"awesome\", \"beautiful\", \"bench\", \"blackwalnut\", \"bring\", \"buy\", \"chair\", \"check\", \"collection\", \"color\", \"colour\", \"congrat\", \"congratulation\", \"dark\", \"decor\", \"deliver\", \"design\", \"dimension\", \"dining\", \"diningchair\", \"dm\", \"fluidart\", \"furnituredesign\", \"furnituresg\", \"geowhee\", \"get\", \"good\", \"gorgeous\", \"grain\", \"greyandsander\", \"greyandsander\", \"greyandsander\", \"homedecor\", \"homedecoration\", \"homedesign\", \"homedesign\", \"homegoal\", \"homeinspiration\", \"homeinspo\", \"homeinterior\", \"ig\", \"igsg\", \"info\", \"instagram\", \"instasg\", \"interior\", \"interior\", \"interiordeco\", \"interiordecor\", \"interiordesign\", \"interiorsg\", \"italian\", \"japanesemethod\", \"jethro\", \"know\", \"know\", \"landedhouse\", \"leather\", \"leathersofa\", \"let\", \"let\", \"livefastdiey\", \"livingroom\", \"livingroomdecor\", \"look\", \"look\", \"love\", \"luxurylifestyle\", \"message\", \"minimalist\", \"much\", \"natural\", \"nice\", \"plan\", \"pm\", \"pm\", \"post\", \"price\", \"s\", \"say\", \"schedule\", \"seater\", \"send\", \"sgart\", \"sghome\", \"sghomedecor\", \"singapore\", \"singaporedecor\", \"size\", \"sofa\", \"sofas\", \"solidwood\", \"style\", \"style\", \"sufianv\", \"table\", \"take\", \"take\", \"thank\", \"time\", \"ve\", \"want\", \"well\", \"wood\", \"woodchair\", \"woodslabstable\", \"woodslabstable\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1323625588510501765258823071\", ldavis_el1323625588510501765258823071_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1323625588510501765258823071\", ldavis_el1323625588510501765258823071_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1323625588510501765258823071\", ldavis_el1323625588510501765258823071_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.328034  0.002497       1        1  71.845750\n",
       "0     -0.159034 -0.124537       2        1  15.430356\n",
       "1     -0.169000  0.122040       3        1  12.723894, topic_info=              Term        Freq       Total Category  logprob  loglift\n",
       "15       homedecor  118.000000  118.000000  Default  30.0000  30.0000\n",
       "156   homeinterior  111.000000  111.000000  Default  29.0000  29.0000\n",
       "216       homegoal  106.000000  106.000000  Default  28.0000  28.0000\n",
       "219    sghomedecor  106.000000  106.000000  Default  27.0000  27.0000\n",
       "217      homeinspo  106.000000  106.000000  Default  26.0000  26.0000\n",
       "..             ...         ...         ...      ...      ...      ...\n",
       "252             pm    2.295326    3.830889   Topic3  -4.8961   1.5495\n",
       "46             let    1.604592    3.127408   Topic3  -5.2541   1.3944\n",
       "33            take    1.415212    2.950754   Topic3  -5.3797   1.3269\n",
       "37            look    1.604562    5.665424   Topic3  -5.2541   0.8002\n",
       "14   greyandsander    2.775214   74.622489   Topic3  -4.7062  -1.2300\n",
       "\n",
       "[130 rows x 6 columns], token_table=      Topic      Freq            Term\n",
       "term                                 \n",
       "360       2  0.715633         artwork\n",
       "422       2  0.715716         awesome\n",
       "171       2  0.878734       beautiful\n",
       "169       2  0.814011           bench\n",
       "160       1  0.990925     blackwalnut\n",
       "...     ...       ...             ...\n",
       "229       2  0.878730            well\n",
       "36        2  0.790482            wood\n",
       "417       1  0.948901       woodchair\n",
       "106       1  0.980664  woodslabstable\n",
       "106       2  0.024517  woodslabstable\n",
       "\n",
       "[108 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now that the LDA model is built, the next step is to examine the produced \n",
    "topics and the associated keywords. There is no better tool than pyLDAvis package‚Äôs \n",
    "interactive chart and is designed to work well with jupyter notebooks.\n",
    "'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSo how to infer pyLDAvis‚Äôs output?\\n\\nEach bubble on the left-hand side plot represents a topic. \\nThe larger the bubble, the more prevalent is that topic.\\n\\nA good topic model will have fairly big, non-overlapping bubbles scattered \\nthroughout the chart instead of being clustered in one quadrant.\\n\\nA model with too many topics, will typically have many overlaps, \\nsmall sized bubbles clustered in one region of the chart.\\n\\nAlright, if you move the cursor over one of the bubbles, \\nthe words and bars on the right-hand side will update. \\nThese words are the salient keywords that form the selected topic.\\n\\nWe have successfully built a good looking topic model.\\n'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So how to infer pyLDAvis‚Äôs output?\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. \n",
    "The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered \n",
    "throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, \n",
    "small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, \n",
    "the words and bars on the right-hand side will update. \n",
    "These words are the salient keywords that form the selected topic.\n",
    "\n",
    "We have successfully built a good looking topic model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'greyandsanders_lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
